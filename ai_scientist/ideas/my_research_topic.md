# Title: Training Only on Good Personas: Preventing the Emergence of Harmful Behaviors in LLMs

## Keywords

LLMs, alignment, personas, pretraining, safety

## TL;DR

What if LLMs were trained only on “good personas”? This workshop explores whether structural prevention of harmful personas could make models safer and more robust.

## Abstract

The goal of this workshop is to spark discussion on whether large language models (LLMs) can be fundamentally safer by design if they are only ever trained on “good personas.” Current safety methods typically suppress harmful behaviors after pretraining, but these personas remain latent and can sometimes be reactivated through adversarial prompts or fine-tuning. By contrast, restricting pretraining to prosocial, constructive personas may prevent the very formation of harmful modes, making them impossible to activate at inference time.

In this year’s workshop, we aim to examine the feasibility, risks, and implications of persona-constrained pretraining. Key questions include: How can we define and filter for “good personas” at internet scale? Would such filtering oversanitize models, limiting creativity, diversity, or robustness? What are the trade-offs between eliminating harmful behaviors and preserving the ability to reason about difficult or sensitive topics? Could this approach serve as a foundation for structural safety guarantees in future models?

We invite contributions from diverse fields including machine learning, alignment research, ethics, cognitive science, data curation, and social sciences. Topics may include novel data-filtering methods, analyses of emergent personas in LLMs, case studies of failure modes arising from harmful role adoption, or philosophical perspectives on what constitutes a “good persona.” By bringing together researchers across disciplines, we hope to better understand both the promise and limitations of persona-constrained training, and to chart a path toward safer, more reliable AI systems.
